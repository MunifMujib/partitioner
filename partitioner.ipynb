{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General message\n",
    "This is the ipython notebook partitioner project development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "$\\cdot$ Collect and preprocess other language dictionarys. <br>\n",
    "$\\cdot$ Incorporate parsing support for other languages. <br>\n",
    "$\\cdot$ Comment the hell out of code <br>\n",
    "$\\cdot$ Build support for TDMs. <br>\n",
    "$\\cdot$ Make lots of examples for the different kinds of functionality. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoreload to make sure we are playing with the most up to date package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./partitioner/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./partitioner/__init__.py\n",
    "__all__ = ['partitioner','methods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./partitioner/partitioner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./partitioner/partitioner.py\n",
    "import re, sys, json, os\n",
    "import math as ma\n",
    "import random as ra\n",
    "import itertools as it\n",
    "\n",
    "class partitioner:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 informed = True, \n",
    "                 qunif = 0.5, \n",
    "                 dictionary = \"NA\", \n",
    "                 qsname = \"enwiktionary\",\n",
    "                 case = False,\n",
    "                 URLS = False,\n",
    "                 hashtags = False,\n",
    "                 handles = False,\n",
    "                 markup = False,\n",
    "                 seed = None\n",
    "                ):\n",
    "        self.home = os.path.dirname(os.path.realpath(__file__))\n",
    "        self.seed = seed\n",
    "        ra.seed(self.seed)\n",
    "        self.case = case\n",
    "        self.URLS = URLS\n",
    "        self.hashtags = hashtags\n",
    "        self.handles = handles\n",
    "        self.markup = markup        \n",
    "        self.informed = informed\n",
    "        self.qunif = qunif\n",
    "        self.dictionary = dictionary\n",
    "        self.qsname = qsname\n",
    "        if self.informed:\n",
    "            if self.dictionary != \"NA\" or self.qsname != \"NA\":\n",
    "                self.loadqs()\n",
    "            else:\n",
    "                print(\"Informed partitions require preprocessed q-probabilities or a dictionary!\")\n",
    "                sys.exit()\n",
    "\n",
    "    def qprob(self,words):\n",
    "        pair = \" \".join(words).lower()\n",
    "        if self.qs.get(pair, False):\n",
    "            return self.qs[pair]\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    def dumpqs(self, qsname):\n",
    "        with open(self.home+\"/../qdumps/\"+qsname+\".json\",\"w\") as f:\n",
    "            f.writelines(json.dumps(self.qs))\n",
    "\n",
    "    def loadqs(self, dictionary = \"NA\", qsname = \"enwiktionary\"):\n",
    "        ## load in the boundary probs from the dictionary\n",
    "        self.qs = {}\n",
    "        if self.dictionary == \"NA\":\n",
    "            ## add switch here to check if preprocessed set exists\n",
    "            try:\n",
    "                with open(self.home+\"/../qdumps/\"+self.qsname+\".json\",\"r\") as f:\n",
    "                    self.qs = json.loads(f.read().strip())\n",
    "            except IOError:\n",
    "                print(\"Preprocessed probabilities for \"+self.qsname+\" have not yet been created!\")\n",
    "                print(\"Place preprocessed probabilities in partitionerPATH/qdumps/,\")\n",
    "                print(\"or load from a dictionay and run partitioner.dumpqs(qsname).\")\n",
    "                sys.exit()\n",
    "        else:\n",
    "            ## load in the boundary probs from the dictionary\n",
    "            left = {}\n",
    "            right = {}\n",
    "            counts = {}\n",
    "            pairs = {}\n",
    "            defined = {}\n",
    "            N = 0.\n",
    "            try:\n",
    "                f = open(self.dictionary,\"r\")\n",
    "            except IOError:\n",
    "                print(\"Specified dictionary does not appear to exist: \"+self.dictionary)\n",
    "                sys.exit()\n",
    "            for phrase in f:\n",
    "                phrase = phrase.strip().lower()\n",
    "                defined[phrase] = 1\n",
    "\n",
    "                words = re.split(\" \",phrase)\n",
    "                counts.setdefault(phrase,{})\n",
    "\n",
    "                left.setdefault(words[-1],[])\n",
    "                left[words[-1]].append(phrase)\n",
    "\n",
    "                right.setdefault(words[0],[])\n",
    "                right[words[0]].append(phrase)    \n",
    "\n",
    "                ## add pairs for this phrase\n",
    "                for i in range(1,len(words)):\n",
    "                    pair = \" \".join(words[i-1:i+1])\n",
    "\n",
    "                    pairs.setdefault(pair,0.)\n",
    "                    pairs[pair] += 1.\n",
    "\n",
    "                    counts[phrase].setdefault(pair,0.)\n",
    "                    counts[phrase][pair] += 1.\n",
    "                N += 1.\n",
    "            f.close()\n",
    "\n",
    "            for pair in pairs:\n",
    "                w1,w2 = re.split(\" \",pair)\n",
    "                k = 0.\n",
    "                PSUM = 0.\n",
    "                loss = 0.\n",
    "                ## any pairs not in loop will contribute 0 to sum,\n",
    "                ## so just need to know how many possible, for the denominator:\n",
    "                ## = f(A B)*N + (N - f(A B))*f(A B) = f(A B)*(2N - f(A B))\n",
    "                ## and then subtract this off by the number covered in the loop\n",
    "                k = pairs[pair]*(2.*N - pairs[pair])\n",
    "                if left.get(w1, False) and right.get(w2, False):\n",
    "                    for L_phrase in left[w1]:\n",
    "                        if counts[L_phrase].get(pair, False):\n",
    "                            L_NUMPAIR = counts[L_phrase][pair]\n",
    "                        else:\n",
    "                            L_NUMPAIR = 0.\n",
    "                        for R_phrase in right[w2]:\n",
    "                            k += 1.\n",
    "                            if counts[R_phrase].get(pair, False):\n",
    "                                R_NUMPAIR = counts[R_phrase][pair]\n",
    "                            else:\n",
    "                                R_NUMPAIR = 0.\n",
    "                            PSUM += 1./(1. + L_NUMPAIR + R_NUMPAIR)\n",
    "                            if L_NUMPAIR + R_NUMPAIR:\n",
    "                                loss += 1.\n",
    "                ## correct for the phrases covered in the loop\n",
    "                k -= loss\n",
    "                self.qs[pair] = PSUM/k\n",
    "    \n",
    "    def washText(self, text):\n",
    "        ## remove additional whitespace\n",
    "        text = re.sub(\"^[ ]+\",\"\",text)\n",
    "        text = re.sub(\"[ ]+$\",\"\",text)\n",
    "        text = re.sub(\"[ ]+\",\" \",text)\n",
    "        ## drop to lower case\n",
    "        if self.case:\n",
    "            text = text.lower()\n",
    "        ## replace URLS with 'http'\n",
    "        if self.URLS:\n",
    "            text = re.sub(\"http[^ \\n]+\",\"http\",text)\n",
    "        ## replace hashtags with '#hash'\n",
    "        if self.hashtags:\n",
    "            text = re.sub(\"\\#[^ ]+\",\"#hash\",text)\n",
    "        ## replace handles with '@handle'\n",
    "        if self.handles:\n",
    "            text = re.sub(\"\\@[^ ]+\",\"@hand\",text)\n",
    "        ## revert common markup back to human readable\n",
    "        if self.markup:\n",
    "            text = re.sub(\"\\&lt\",\"\\<\",text)\n",
    "            text = re.sub(\"\\&gt\",\"\\>\",text)\n",
    "            text = re.sub(\"\\&amp\",\"\\&\",text)\n",
    "            text = re.sub(\"\\\\n\",\"\\n\",text)\n",
    "            text = re.sub(\"\\\\t\",\"\\t\",text)\n",
    "        return text\n",
    "    \n",
    "    def testFit(self):\n",
    "        self.rsq = \"NA\"\n",
    "        sizes = {}\n",
    "        for phrase in self.counts:\n",
    "            count = self.counts[phrase]\n",
    "            sizes.setdefault(count,0)\n",
    "            sizes[count] += 1\n",
    "        pairs = [[size,sizes[size]] for size in sizes]\n",
    "        N = 0.0\n",
    "        M = 0.0\n",
    "        cumNumbers = []\n",
    "        cumSizes = []\n",
    "        for size,number in sorted(pairs,key=lambda x: x[0],reverse=True):\n",
    "            N += float(number)\n",
    "            M += float(size)*float(number)\n",
    "            cumNumbers.append(N)\n",
    "            cumSizes.append(float(size))\n",
    "        if M:\n",
    "            ## Zipf/Simon model fit:\n",
    "            m = -(1. - N/M)\n",
    "            b = -m*ma.log(N,10)\n",
    "            f = [(ma.log(cumSizes[i],10)-b)/m for i in range(len(cumSizes))]\n",
    "            r = [ma.log(cumNumbers[i],10) - f[i] for i in range(len(cumNumbers))]\n",
    "            fmean = sum(f)/float(len(f))\n",
    "            mss = sum([(f[i] - fmean)**2.0 for i in range(len(f))])\n",
    "            rss = sum([r[i]**2.0 for i in range(len(r))])\n",
    "            self.rsq = mss/(mss + rss)\n",
    "        else:\n",
    "            print(\"There's no data on which to test a fit!\")\n",
    "            sys.exit()\n",
    "            \n",
    "    def partitionText(self,text = \"\", textfile = \"NA\"):\n",
    "        ## set things up\n",
    "        reg = re.compile(\"((\\#|\\@)?[a-zA-Z]+((\\'|\\-)[a-zA-Z]+)*\\'?[ ]?)+\")\n",
    "        self.counts = {}\n",
    "        if textfile != \"NA\":\n",
    "            try:\n",
    "                with open(textfile, \"r\") as f:\n",
    "                    text=f.read()\n",
    "            except IOError:\n",
    "                print(\"Specified text file does not appear to exist: \"+textfile)\n",
    "                sys.exit()            \n",
    "        text = self.washText(text)\n",
    "        ## count the words/phrases\n",
    "        for clause in reg.finditer(text):\n",
    "            clause = clause.group()\n",
    "            partition = self.partition(clause)\n",
    "            if type(partition) is dict:\n",
    "                for phrase in partition:\n",
    "                    self.counts.setdefault(phrase,0.)\n",
    "                    self.counts[phrase] += partition[phrase]\n",
    "            else:\n",
    "                for phrase in partition:\n",
    "                    self.counts.setdefault(phrase,0.)\n",
    "                    self.counts[phrase] += 1.\n",
    "                    \n",
    "    def partition(self, text = \"\", textfile = \"NA\"):\n",
    "        print(\"Partition method not yet set!\")\n",
    "        print(\"To partition text, import and run 'oneoff' or 'stochastic'.\")\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./partitioner/methods.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./partitioner/methods.py\n",
    "import re, sys, json, os\n",
    "import math as ma\n",
    "import random as ra\n",
    "import itertools as it\n",
    "from partitioner import partitioner\n",
    "\n",
    "class stochastic(partitioner):\n",
    "    \n",
    "    def testFit(self):\n",
    "        print(\"Can't test fit goodness on non-deterministic partitions!\")\n",
    "        print(\"Import and run 'oneoff' to test goodness of fit.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    def partition(self, clause):\n",
    "        words = re.split(\" \", clause)\n",
    "        counts = {}\n",
    "        if self.informed:\n",
    "            ## get the probs:\n",
    "            qs = []\n",
    "            for i in range(1,len(words)):\n",
    "                qs.append(self.qprob(words[i-1:i+1]))\n",
    "        \n",
    "        orders = [len(words) + 1 - k for k in range(1,len(words)+1)]\n",
    "        for order in orders:    \n",
    "            for start in range(len(words) - order + 1):\n",
    "                end = start + order\n",
    "                \n",
    "                fq = 0.\n",
    "                possible = 1\n",
    "                if self.informed:\n",
    "                    if start:\n",
    "                        q = qs[start-1]\n",
    "                        if q:\n",
    "                            fq += ma.log(q,2.)\n",
    "                        else:\n",
    "                            possible = 0\n",
    "                    if end - len(words):\n",
    "                        q = qs[end-1]\n",
    "                        if q:\n",
    "                            fq += ma.log(q,2.)\n",
    "                        else:\n",
    "                            possible = 0\n",
    "\n",
    "                    for i in range(1,len(words[start:end])):\n",
    "                        q = qs[start:end-1][i-1]\n",
    "                        if 1. - q:\n",
    "                            fq += ma.log(1. - q, 2.)\n",
    "                        else:\n",
    "                            possible = 0\n",
    "                            break\n",
    "                else:\n",
    "                    bnd = 0.\n",
    "                    if not start:\n",
    "                        bnd += 1.\n",
    "                    if end == len(words):\n",
    "                        bnd += 1.\n",
    "                    if 1. - self.qunif:\n",
    "                        fq += (order-1.)*ma.log(1. - self.qunif,2.) \n",
    "                    if self.qunif:\n",
    "                        fq += (2. - bnd)*ma.log(self.qunif,2.)\n",
    "\n",
    "                if possible:\n",
    "                    fq = 2. ** fq\n",
    "                    phrase = \" \".join(words[start:end])\n",
    "                    counts.setdefault(phrase,0.)\n",
    "                    counts[phrase] += fq\n",
    "                else:\n",
    "                    fq = 0.    \n",
    "        return counts\n",
    "\n",
    "class oneoff(partitioner):\n",
    "    \n",
    "    def partition(self, clause):\n",
    "        partition = []\n",
    "        words = clause.split(\" \")\n",
    "        length = len(words)\n",
    "        if length - 1:\n",
    "            randNums = [ra.random() for i in range(length-1)]\n",
    "            ## compute the 1-off partition for qInf here\n",
    "            start = 0\n",
    "            order = 1\n",
    "            ends = []\n",
    "            end = 1\n",
    "            for randNum in randNums:\n",
    "                if self.informed:\n",
    "                    pprob = self.qprob(words[end-1:end+1])\n",
    "                    if randNum <= pprob:\n",
    "                        ends.append(end)\n",
    "                else:\n",
    "                    if randNum <= self.qunif:\n",
    "                        ends.append(end)\n",
    "                end += 1\n",
    "            ends.append(end)\n",
    "            start = 0\n",
    "            for end in ends:\n",
    "                phrase = str(\" \".join(words[start:end]))\n",
    "                partition.append(phrase)\n",
    "                start = end\n",
    "        else:\n",
    "            phrase = str(words[0])\n",
    "            partition.append(phrase)\n",
    "        return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./examples/examples.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./examples/examples.py\n",
    "from partitioner import partitioner\n",
    "from partitioner.methods import *\n",
    "## This file scripts partitioner examples,\n",
    "## which are to be run as standalone vignettes\n",
    "## exhibiting partitioner's functionality\n",
    "\n",
    "## Vignette 1: Build informed partition data from a dictionary, \n",
    "##             and store in a local repo\n",
    "def preprocessENwiktionary():\n",
    "    pa = partitioner(informed = True, dictionary = \"./dictionaries/enwiktionary.txt\")\n",
    "    pa.dumpqs(qsname=\"enwiktionary\")\n",
    "    \n",
    "## Note: Cleaning of text and determination of clauses\n",
    "##       occurs in the partitionText method. \n",
    "##       Because of this, \n",
    "##       it is unwise to pass large, uncleaned pieces of text as 'clauses' \n",
    "##       directly through the .partition() method (regardless of the type of partition being taken),\n",
    "##       as this will simply tokenize the text by splitting on \" \", \n",
    "##       producing many long, punctuation-filled phrases, \n",
    "##       and likely run very slow.\n",
    "##       As such, best practices only use .partition()\n",
    "##       for testing and exploring the tool on case-interested clauses.\n",
    "    \n",
    "## Vignette 2: An informed, one-off partition of a single clause\n",
    "def informedOneOffPartition(clause = \"How are you doing today?\"):\n",
    "    pa = oneoff()\n",
    "    print pa.partition(clause)\n",
    "\n",
    "## Vignette 3: An informed, stochastic partition of a single clause\n",
    "def informedStochasticPartition(clause = \"How are you doing today?\"):\n",
    "    pa = stochastic()\n",
    "    print pa.partition(clause)\n",
    "    \n",
    "## Vignette 4: An uniform, one-off partition of a single clause\n",
    "def uniformOneOffPartition(informed = False, clause = \"How are you doing today?\", qunif = 0.25):\n",
    "    pa = oneoff(informed = informed, qunif = qunif)\n",
    "    print pa.partition(clause)\n",
    "\n",
    "## Vignette 5: An informed, stochastic partition of a single clause\n",
    "def uniformStochasticPartition(informed = False, clause = \"How are you doing today?\", qunif = 0.25):\n",
    "    pa = stochastic(informed = informed, qunif = qunif)\n",
    "    print pa.partition(clause)\n",
    "    \n",
    "## Vignette 6: Use the default partitioning method to partition the main partitioner.py file and compute rsq\n",
    "def testPartitionTextAndFit():\n",
    "    pa = oneoff()\n",
    "    pa.partitionText(textfile = pa.home+\"/partitioner.py\")\n",
    "    pa.testFit()\n",
    "    print pa.rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
